# coding:utf-8
# _*_coding:utf-8 _*_
# @Time　　 : 2020/03/08   0:00
# @Author　 : zimo
# @File　   :
# @Software :PyCharm


# 聚合一组模型的预测，得到的预测结果通常比单个模型的效果要好。这样一组模型称为集成，所以这种技术，也被称为集成学习。

# 即使每个分类器都是弱学习器，通过集成依然可以实现一个强学习器(有高的准确率)。

# 构造模型与数据的多样性
# ■不同的分类器算法:
# ■选择不同的超参数
# ■随机采样数据
# ■随机选择特征

#
# 集成学习算法：
# Bagging，有采样的放回，随机森林
# Boosting，带权重的的投票，GBDT


随机森林( Random Forest )

1、从原始训练数据集中,应用botstrap方法有放回地随机抽取k个新的子样本集，并由此构建k棵分类回归树,每次未被抽到的样本组成了k个袋外数据(out-of-bag, BBB )。

2、设有n个特征,则在每一棵树的每个节点处随机抽取m个特征子集,在随机生成的特征子集里搜索最好的特征(CART/信 息增益)。

3、每棵树最大限度地生长，不做任何剪裁。

4、将生成的多棵树组成随机森林，用随机森林对新的数据进行
分类，分类结果按树分类器投票多少而定。



0森林中单颗树的分类强度(Strength) :每颗树的分类强度越大,则随机森林的
分类性能越好
0森林中树之间的相关度(Correlation) :树之间的相关度越大,则随机森林的分类性能越差
0继承了决策树的优点,同时提高了泛化能力,并且可以并行训练单棵树。
0 OOB包外评估错误率是随机森林的错误率无偏估计每颗决策树在其包外实例上的评估结果进行平均，可以得到对随机森林的
评估,;因此随机森林可以不需要进行交叉验证


# 随机森林模型
from sklearn.ensemble import RandomForestClassifier
rnd_clf=RandomForestClassifier(n_estimators=500,max_leaf_nodes=16,n_jobs=1,random_state=42)
rnd_clf.fit(X_train,y_train)
y_pred_rf=rnd_clf.predict(X_test)


Boosting
bootsing的总体思路是循环训练多个分类器,每一-次都对前面一个做出一-些改正
目前最流行的方法是AdaBoost(adaptive Boosting自适应提升法)和GBDT(Gradient Boosting梯度提升法)


Adaboost
更多的关注前一个模型拟合不足的训练样本1从而使后面的模型更专注于难缠的问题
算法步骤:
1.训练一个基分类器(比如决策树)
2.增加误分类的样本的权重....
3.使用最新的带权重样本训练第二个模型
4.继续更新权重,不断循环
5.最终结果由这些模型加权投票得出

Adaboost
步骤1.首先，初始化训练数据的权值分布。每一个训练样本最开始权重都被设置I/N,其中N为样本个数。


步骤2.进行多轮迭代，用m= 1,2, ... M表示迭代的第多少轮
a.使用带权重的样本D,作为训练数据集学习,得到基本分类器
(选取让误差率最低的阈值来设计基本分类器) :
Gm(x) x→{-1,+1}
b.计算基本分类器Gm(x)在训练数据集上的分类误差率
Note: Gm(x)在训练数据集上的误差率e,就是被Gm(x)错误分类样本的权值之和。

步骤3.计算分类器Gm(x)的权重, am表示Gm(x)在最终分类器中的重要程度:

Note:em <= 0.5时，am>=0,且am随着em的减小而增大，意味着分类误差率越小的基本分类器在最终分类器中的作用越大。当em =0.5. Xm =0,相当于随机分类。

